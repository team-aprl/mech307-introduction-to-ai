\documentclass[12pt]{article}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{enumitem}
\usepackage{geometry}
\geometry{margin=1in}

\usepackage{url}

\usepackage{listings}
\usepackage{xcolor}

\lstdefinestyle{mypython}{
  language=Python,
  backgroundcolor=\color{gray!10},   % 연한 회색 배경
  basicstyle=\ttfamily\footnotesize, % 폰트 스타일
  keywordstyle=\color{blue},         % 키워드 색
  stringstyle=\color{red},           % 문자열 색
  commentstyle=\color{green!50!black}, % 주석 색
  numberstyle=\tiny\color{gray},     % 줄번호 색
  numbers=left,                      % 줄번호 왼쪽에 표시
  stepnumber=1,                      % 한 줄마다 번호
  breaklines=true,                   % 줄 길면 자동 줄바꿈
  frame=single,                      % 테두리 박스
  rulecolor=\color{black}            % 테두리 색
}

\title{MECH307: Introduction to Artificial Intelligence \\ Homework \#1}
\author{Instructor: Giseop Kim}
\date{Due: September 28, 2025, 11:59 PM}

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Instructions}
\begin{itemize}
    \item Submit your answers as a single PDF file.
    \item Solutions may be written in \LaTeX, Markdown (with math), or handwritten on paper (scanned).
    \item Show all intermediate steps and reasoning for full credit.
    \item Code problems must include both the code and the generated results (plots, printed output).
    \item External resources (e.g., Google, ChatGPT) are allowed \emph{after} you write down your own initial thoughts.
    \item Late submission policy: if the homework is submitted within 1 minute to 24 hours after the deadline, the total score will be reduced by 20\%. 
    Submissions more than 24 hours late will receive 0 points.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\section*{Problems}

\begin{enumerate}[label=\textbf{Q\arabic*.}]

% =========================
% Q1: Embedding Experiment
% =========================
\item \textbf{Embedding Experiment \& Analysis} (25 pt)

We will explore word embeddings using the \texttt{sentence-transformers} library.

\begin{enumerate}[label=(\alph*)]

    \item \textbf{Run the provided code.} (5 pt)  
    Open and run this code\footnote{\url{https://github.com/team-aprl/mech307-introduction-to-ai/blob/main/codes/semantic_arithmetic.py}}. This code is about the semantic analogy experiment.
    \[
        \texttt{king} - \texttt{man} + \texttt{woman} \approx ?
    \]
    You can use Google Colab. A result screenshot capture should be attached. 
 
    \item \textbf{Try three different embedders.} (5 pt)  
    In the code, you will find the model loading line:
    \[
        \texttt{model = SentenceTransformer("all-MiniLM-L6-v2")}
    \]
    Replace the given embedder with three other models supported in the \\ \texttt{sentence-transformers} library.  
    For each model, report the cosine similarity scores for the given \texttt{candidates} words.   

    \item \textbf{Guess and Reflect.} (10 pt)  
    The results across different embedders may not be the same.  
    Based on your experiments, \emph{guess} why these differences occur and how one might decide which embedder is ``better’’. You don’t need a fully correct answer—just write your own reasoning

    \item \textbf{Design Your Own Semantic Arithmetic.} (5 pt)  
    In the class, we saw an example like
    \[
        \texttt{king} - \texttt{man} + \texttt{woman} \approx \texttt{queen}.
    \]
    Propose \emph{one new semantic arithmetic example} of your own.  
    Run the experiment with your chosen embedder(s), report the similarity results, and discuss whether the outcome makes sense.

\end{enumerate}


% =========================
% Q2: Autodiff
% =========================
\item \textbf{Automatic Differentiation (Analytic vs.\ PyTorch)} (25 pt)

We consider two examples to compare analytic gradients and automatic differentiation.

\begin{enumerate}[label=(\alph*)]
    \item \textbf{Example 1: Nonlinear Function} (10 pt)  
    \[
        f(x, y) = e^x \sin(y), \quad x=0, \, y=\tfrac{\pi}{4}.
    \]
    \begin{enumerate}[label=(\roman*)]
        \item Derive $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$ analytically. (4 pt)
        \item Implement in PyTorch with \verb|requires_grad=True| and confirm via \verb|.backward()|. (4 pt)
        \item Compare those two results. (2 pt)
    \end{enumerate}

    \item \textbf{Example 2: Linear Regression with MSE Loss} (15 pt)  
    \[
        \hat{y} = w x + b, 
        \quad 
        \ell(w,b) = (\hat{y} - y)^2,
        \qquad (x,y,w,b)=(2,5,1,0).
    \]
    \begin{enumerate}[label=(\roman*)]
        \item Derive $\frac{\partial \ell}{\partial w}$ and $\frac{\partial \ell}{\partial b}$ analytically. (5 pt)
        \item Verify with PyTorch autograd. (5 pt)
        \item Briefly discuss why autodiff is useful in deep learning practice. (5 pt)
    \end{enumerate}
\end{enumerate}

% =========================
% Q3: MLP on MNIST vs. Another Dataset
% =========================
\item \textbf{MLP on a New Dataset (Reflection)} (25 pt)

Using the provided \texttt{mlp\_mnist.py} code\footnote{\url{https://github.com/team-aprl/mech307-introduction-to-ai/blob/main/codes/mlp_mnist.py}}, train the MLP model on \textbf{either Fashion-MNIST \emph{or} Tiny ImageNet}, in addition to MNIST.  
You may freely modify the model structure or hyperparameters while conducting your experiments.

If you choose \textbf{Fashion-MNIST}, note that it is a drop-in replacement for MNIST with the same image size (28$\times$28 grayscale), 
but contains fashion item categories (e.g., shirts, shoes, bags) instead of handwritten digits.  
You can load it directly from \texttt{torchvision.datasets.FashionMNIST} with the same normalization as MNIST.  

If you choose \textbf{Tiny ImageNet}, see the dataset description at:  
\url{https://www.kaggle.com/c/tiny-imagenet}  
You can download it directly at Colab with :
\begin{lstlisting}[style=mypython]
# ------------------ Download & Extract Tiny ImageNet ------------------
!wget -nc http://cs231n.stanford.edu/tiny-imagenet-200.zip
!unzip -q -n tiny-imagenet-200.zip
\end{lstlisting}

After running for a few epochs, write a short reflection on what you observe about training difficulty, accuracy, and dataset complexity. You are free to include plots or tables.


% =========================
% Q4: Linear Regression with Noisy Inputs
% =========================
\item \textbf{Linear Regression with Gaussian Noise} (25 pt)

Consider a linear regression model
\[
y = \phi_0 + \phi_1 x
\]
with input $x$, output $y$, and parameters $\phi_0, \phi_1$. Assume we have $I$ training examples $\{x_i, y_i\}$ and use a least squares loss.  
At each training iteration, Gaussian noise with mean zero and variance $\sigma_x^2$ is added to the inputs $x_i$.  

\begin{enumerate}[label=(\alph*)]
    \item Derive an expression for the \emph{expected loss} (the expectation of the loss) under this noisy input setting. Show the intermediate steps. (15 pt)
    \item Explain in your own words what effect this kind of input noise has during model training. (10 pt)
\end{enumerate}

\end{enumerate}
\end{document}
